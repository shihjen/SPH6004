{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ff1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# helper function to compute Cohen's d value\n",
    "def cohen_d(group1, group2):\n",
    "    mean_diff = np.mean(group1) - np.mean(group2)\n",
    "    pooled_std = np.sqrt((np.std(group1, ddof=1) ** 2 + np.std(group2, ddof=1) ** 2) / 2)\n",
    "    \n",
    "    return mean_diff / pooled_std\n",
    "\n",
    "# helper function to compute p-value and effect size of the features among 2 groups\n",
    "def performStat(group1,group2,selected_feats,alternative='two-sided',effectSize=0.1):\n",
    "    featDiff = []\n",
    "    largeEffect = []\n",
    "    test_stat_col = []\n",
    "    pvalue_col = []\n",
    "    effect_size_col = []\n",
    "    for feat in selected_feats:\n",
    "        res = mannwhitneyu(group1[feat], group2[feat], alternative=alternative)\n",
    "        test_stat = res[0]\n",
    "        test_stat_col.append(test_stat)\n",
    "        pvalue = round(res[1],5)\n",
    "        pvalue_col.append(pvalue)\n",
    "        if pvalue > 0.05:\n",
    "            featDiff.append(feat)\n",
    "        # compute effect size (Cohen's d)\n",
    "        effect_size = round(cohen_d(group1[feat],group2[feat]), 4)\n",
    "        effect_size_col.append(effect_size)\n",
    "        if abs(effect_size) > effectSize:\n",
    "            largeEffect.append(feat)\n",
    "            \n",
    "    analysisResult = pd.DataFrame([test_stat_col,pvalue_col,effect_size_col]).T\n",
    "    analysisResult.columns = ['Test Statistic', 'Test p-value','Effect Size']\n",
    "    analysisResult.index = selected_feats\n",
    "    display(analysisResult)\n",
    "    print(f'{len(featDiff)} features have p-value greater than 0.05, which is not significant different between 2 groups.')\n",
    "    print(f'{len(largeEffect)} features have effect size greater than {effectSize}.')\n",
    "    return largeEffect\n",
    "\n",
    "def errorAnalysis(model,model_training_feature_set,X,y,effectSize=0.3):\n",
    "    ypred_train = model.predict(X[model_training_feature_set])\n",
    "    ypred_prob_train = model.predict_proba(X[model_training_feature_set])\n",
    "    neg_pred = [prob[0] for prob in ypred_prob_train]\n",
    "    pos_pred = [prob[1] for prob in ypred_prob_train]\n",
    "\n",
    "    analysis = pd.DataFrame({'True':y,\n",
    "                             'Predicted':ypred_train,\n",
    "                             'Predict_NoAKI':neg_pred,\n",
    "                             'Predict_AKI':pos_pred\n",
    "                            })\n",
    "\n",
    "    # dataframe with only features selected from genetic algorithm and the predicted value, prediction probablities\n",
    "    analysis_df = pd.concat([Xtrain_processed[model_training_feature_set],analysis], axis=1)\n",
    "\n",
    "    # view the first 5 rows of the analysis_df dataframe\n",
    "    display(analysis_df.head())\n",
    "    \n",
    "    # obtain the index of the incorrect prediction\n",
    "    incorrect_index = []\n",
    "    for i in range(analysis_df.shape[0]):\n",
    "        if analysis_df['True'][i] != analysis_df['Predicted'][i]:\n",
    "            incorrect_index.append(i)\n",
    "\n",
    "    # check the total number of incorrect prediction\n",
    "    print('{} records are incorrectly predicted.'.format(len(incorrect_index)))\n",
    "    \n",
    "    # split the dataframe into 2\n",
    "    # dataframe 1 - correctly predicted, named as 'correct_prediction'\n",
    "    # dataframe 2 - incorrectly predicted, named as 'incorrect_prediction'\n",
    "\n",
    "    # correct prediction\n",
    "    correct_prediction = analysis_df[~analysis_df.index.isin(incorrect_index)]\n",
    "    display(correct_prediction.head())\n",
    "    print('Correct prediction:')\n",
    "    print('Number of correct prediction: ', correct_prediction.shape[0])\n",
    "\n",
    "    # incorrect prediction\n",
    "    incorrect_prediction = analysis_df[analysis_df.index.isin(incorrect_index)]\n",
    "    display(incorrect_prediction.head())\n",
    "    print('Incorrect prediction:')\n",
    "    print('Number of incorrect prediction: ', incorrect_prediction.shape[0])\n",
    "    print()\n",
    "    print('###########################################################################################################')\n",
    "    \n",
    "    # split the dataset into true positive, true negative, false positive & false negative\n",
    "    correct = correct_prediction.groupby('True')\n",
    "    true_positive = correct.get_group(1)\n",
    "    true_negative = correct.get_group(0)\n",
    "\n",
    "    incorrect = incorrect_prediction.groupby('True')\n",
    "    false_positive = incorrect.get_group(0)\n",
    "    false_negative = incorrect.get_group(1)\n",
    "\n",
    "    # check the number of true positive, true negative, false positive, false negative\n",
    "    print('\\nNumber of True Positive: ',true_positive.shape[0])\n",
    "    print('Number of True Negative: ',true_negative.shape[0])\n",
    "    print('Number of False Positive: ',false_positive.shape[0])\n",
    "    print('Number of False Negative: ',false_negative.shape[0])\n",
    "    print()\n",
    "    print('###########################################################################################################')  \n",
    "    \n",
    "    # comparison between True Positive & True Negative\n",
    "    print('\\nComparison of the effect size of selected features in True Positive & True Negative Classes:')\n",
    "    performStat(true_positive, true_negative, model_training_feature_set, alternative='two-sided',effectSize=effectSize)\n",
    "    \n",
    "    print()\n",
    "    print('###########################################################################################################')\n",
    "    \n",
    "    # comparison between True Positive & False Positive\n",
    "    print('\\nComparison of the effect size of selected features in True Positive & False Positive Classes:')\n",
    "    performStat(true_positive, false_positive, model_training_feature_set, alternative='two-sided',effectSize=effectSize)\n",
    "    \n",
    "    print()\n",
    "    print('###########################################################################################################')\n",
    "    \n",
    "    # comparison between True Negative & False Negative\n",
    "    print('\\nComparison of the effect size of selected features in True Negative & False Negative Classes:')\n",
    "    performStat(true_negative, false_negative, model_training_feature_set, alternative='two-sided',effectSize=effectSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
